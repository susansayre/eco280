---
title: "Summary statistics"
author: "Susan Sayre"
date: "10/21/2020"
weight: 7
always_allow_html: true
output: 
  md_document:
    preserve_yaml: true
    pandoc_args: ["--wrap=preserve"]
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(modelsummary)
library(palmerpenguins)
```

When you begin working with new data, one of your important tasks is to "get to know your data". To be thorough and careful, you will often want to do this with **both** your raw data as soon as you read it in **and** your final data once you have processed and "wrangled" it. This can help make sure that you have not introduced bias through processing your data. For instance, if a key merging variable is missing for many observations that are systematically different than the rest of the population, there will be noticeable differences between summary statistics for your raw data and your processed data. Sometimes this is unavoidable, but you should be aware of (and discuss in your work) the possible implications.

## Summary statistics for numerical variables

You should compute summary statistics for each of your numeric variables. At a minimum, your summary statistics should include the mean, standard deviation, max, and mean for each variable. In addition to these, I often suggest looking at the median and the 25th and 75th percentile.

Once you have created the summary statistics, **look at them closely.** Do all of the means make sense? What about the maximum and minimum values? If you've looked at the medians and the percentiles, do you see evidence of a skewed distribution. Consider plotting histograms for the variables. Think about what the distribution of the data might imply for how you plan to use it.

## Frequency counts for categorical variables

Categorical variables in your data come in two different types: ones with "a few" possible values and ones with "lots" of possible values. It is generally a good idea to look at frequency counts for your categorical variables in one of two ways:

- For categorical variables with a "few" possible values, compute standard frequency counts that tell you the number of observations in each category. You should make sure to determine not only the number of observations with each possible value, but also the number of observations for which the value is missing. Depending on your purpose, you may choose to present either the raw counts, the percentage of observations, or both.

- For some categorical variables with a "few" observations, you may want to report cross-tab frequencies. For instance, if you had variables for employment status and marital status, you might find the cross-tabs telling you whether unemployment status looks different for married individuals vs unmarried individuals interesting. There is no hard and fast rule on when you should compute cross-tabs; it depends on your research question and whether you think the cross-tabs will matter. A common case where you probably *should* include cross-tabs is when you are testing the impact of a categorical variable.

- For variables with "lots" of possible values, it is generally impractical to report full frequency counts. Instead, I often find it useful to compute frequency counts of the frequency counts. That is, I will first compute a standard frequency count of how often each categorical value appears in the dataset. Then I will compute frequency counts of the frequency values -- e.g. how many categorical values appear once, twice, 10 times, etc. This information is not always useful but could be. As with cross-tabs, there is no hard and fast rule that defines what constitutes a variable with "few" values vs one with "lots" of values, nor is there is a clear indication of when it is important to look at frequency counts of frequency counts versus not summarizing the variables with many observations. I generally do the latter when it will cause problems for my analysis if the frequency of different frequencies within the data will matter. 

## Commands for summarizing data

### R

#### Manually using tidyverse summarize

All of the summary statistics we might want to look at can be manually computed via tidyverse. This works well for numeric variables and is kind of a pain for categorical variables since you have to run each computation manually (or do some fancy re-arranging of your data with pivot_longer). Given this and the importance of summary statistics, there are several packages that make nice summary statistic tables. 

#### Using the vtable package

A simple lightweigt R package for creating summary statistic tables is Nick Huntington-Klein's `vtable` package. As he says "if you want the kind of table sumtable() produces (and I think a lot of you do!) then itâ€™s perfect and easy." I find that it is rare that this package **doesn't** give me what I'm looking for. It is simple, fast, and usually does what you want. There are options to customize many things (although possibly not everything you might want). Here's a basic example:

```{r simple sum stat table}

st(penguins)

```

With a couple more lines of code, I could move the numeric variables to the top and make a slighter nicer table.

```{r numeric variables at the top}

palmerpenguins::penguins %>% 
  relocate(where(is.numeric)) %>% 
  st()

```

Finally, we could group the data by one of our categorical variables to get summary statistics by group.
```{r by group}

penguins %>% 
  mutate(year = as.factor(year)) %>% 
  relocate(where(is.numeric)) %>% 
  st(group = "species")

```

#### Using the modelsummary package

While I find `vtable` to be a great quick package, the modelsummary package offers more options and is also useful for summarizing regression output.

To get a quick summary of the numeric data, we use datasummary_skim().

```{r}
datasummary_skim(penguins)
```

```{r}
datasummary_skim(penguins, type = "categorical")
```

### Stata

While it is definitely possible to write nice summary statistics. 